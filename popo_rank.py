"""
POPO æ’è¡Œæ¦œçˆ¬èŸ²
----------------
â€¢ å°‡åŸæœ¬åˆ†æ•£åœ¨ `setup_driver()` / `main()` çš„æµç¨‹æ•´åˆé€² `run_crawler()`ï¼Œ
  æ–¹ä¾¿ Streamlit ç›´æ¥å‘¼å«ã€‚
â€¢ æ–°å¢ `progress_callback` åƒæ•¸ï¼Œå¯å³æ™‚æŠŠé€²åº¦æ–‡å­—å‚³åˆ°å‰ç«¯
  ï¼ˆæœªå‚³å…¥æ™‚é è¨­ç‚º `print`ï¼Œæ•…æœ¬åœ°åŸ·è¡Œä¸å—å½±éŸ¿ï¼‰ã€‚
â€¢ å…¶é¤˜æ ¸å¿ƒé‚è¼¯ï¼ˆåˆ‡æ¦œã€è§£æã€å­˜ Excelï¼‰ä¸è®Šã€‚
"""

from __future__ import annotations

import time
from datetime import datetime
from pathlib import Path
from typing import Callable, Optional, Dict, List

import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

# -----------------------------------------------------------------------------
# å…±ç”¨å·¥å…·
# -----------------------------------------------------------------------------

def _default_logger(msg: str) -> None:  # é è¨­ç›´æ¥ print
    print(msg)


def _create_driver() -> webdriver.Chrome:
    """å»ºç«‹ headless Chromeï¼Œé›²ç«¯/æœ¬åœ°çš†å¯ç”¨ã€‚"""
    opts = Options()
    opts.binary_location = "/usr/bin/chromium-browser"  # â˜…æ–°å¢ï¼šé›²ç«¯å®¹å™¨çš„ chromium è·¯å¾‘
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")

    return webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=opts,
    )

# -----------------------------------------------------------------------------
# è§£æå–®æœ¬æ›¸è©³æƒ…
# -----------------------------------------------------------------------------

def get_book_detail(driver: webdriver.Chrome, book_url: str) -> Dict[str, str]:
    driver.get(book_url)
    time.sleep(2)

    # è‡ªå‹•é€šé 18 ç¦é é¢
    if "/limit18" in driver.current_url or "æˆ‘å·²æ»¿18æ­²" in driver.page_source:
        try:
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "a.R-yes"))
            ).click()
            time.sleep(2)
        except Exception as e:
            print(f"[è­¦å‘Š] 18 ç¦é©—è­‰è‡ªå‹•é»æ“Šå¤±æ•—ï¼š{e}")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    tables = soup.find_all("table", class_="book_data")

    detail: Dict[str, str] = {}
    for table in tables:
        for tr in table.find_all("tr"):
            th = tr.find("th")
            td = tr.find("td")
            if th and td:
                detail[th.text.strip()] = td.text.strip()

    return {
        "å…è²»ç« å›": detail.get("å…è²»ç« å›", ""),
        "ä»˜è²»ç« å›": detail.get("ä»˜è²»ç« å›", ""),
        "ç¸½å­—æ•¸": detail.get("ç¸½å­—æ•¸", ""),
        "æ”¶è—æ•¸": detail.get("æ”¶è—æ•¸", ""),
        "è¨‚è³¼æ•¸": detail.get("è¨‚è³¼æ•¸", ""),
    }

# -----------------------------------------------------------------------------
# åˆ‡æ›æ¦œå–® / é€±æœŸè¼”åŠ©
# -----------------------------------------------------------------------------

def switch_board_and_category(driver: webdriver.Chrome, kind: str, sub: str) -> None:
    driver.get("https://www.popo.tw/rank/more")
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, "rank-form1")))
    time.sleep(1)
    js = f"""
    document.getElementById('kind').value ='{kind}';
    document.getElementById('sub').value ='{sub}';
    document.getElementById('rank-form1').submit();
    """
    driver.execute_script(js)
    time.sleep(2)


def switch_rank(driver: webdriver.Chrome, period: str = "weekly") -> None:
    js = f"""
    document.getElementById('type').value='{period}';
    document.getElementById('rank-form1').submit();
    """
    driver.execute_script(js)
    time.sleep(2)

# -----------------------------------------------------------------------------
# çˆ¬å–å–®å¼µæ¦œå–®
# -----------------------------------------------------------------------------

def crawl_board(
    driver: webdriver.Chrome,
    kind: str,
    kind_name: str,
    sub: str,
    sub_name: str,
    period: str,
    period_name: str,
    logger: Callable[[str], None],
) -> pd.DataFrame:
    # åˆ‡æ¦œ + åˆ‡é€±æœŸ
    switch_board_and_category(driver, kind, sub)
    switch_rank(driver, period)

    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "table-rwd"))
    )
    soup = BeautifulSoup(driver.page_source, "html.parser")
    table = soup.find("table", class_="table-rwd")
    rows = table.find("tbody").find_all("tr")

    data: List[Dict[str, str]] = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) == 7:
            book_link = cols[2].find("a", class_="bname")["href"]
            book_url = "https://www.popo.tw" + book_link
            base_info = {
                "æ’è¡Œ": cols[0].text.strip(),
                "é¡åˆ¥": cols[1].text.strip(),
                "æ›¸å": cols[2].text.strip(),
                "æ›¸ç±é€£çµ": book_url,
                "æœ€æ–°ç« å›": cols[3].text.strip(),
                "ä½œè€…": cols[4].text.strip(),
                "å…¬é–‹æ™‚é–“": cols[5].text.strip(),
                "æ›¸ç±ç‹€æ…‹": cols[6].text.strip(),
                "æ¦œå–®": kind_name,
                "åˆ†é¡": sub_name,
                "é€±æœŸ": period_name,
            }
            detail_info = get_book_detail(driver, book_url)
            base_info.update(detail_info)
            data.append(base_info)
            logger(f"âœ” å·²å®Œæˆï¼š{base_info['æ›¸å']} ({kind_name}-{sub_name}-{period_name})")

    return pd.DataFrame(data)

# -----------------------------------------------------------------------------
# å°å¤–ä¸»è¦å…¥å£ â”€â”€ run_crawler()
# -----------------------------------------------------------------------------

def run_crawler(progress_callback: Optional[Callable[[str], None]] = None) -> str:
    """åŸ·è¡Œå®Œæ•´çˆ¬èŸ²ï¼Œä¸¦å°‡éç¨‹è¨Šæ¯é€é callback å‚³éã€‚"""

    logger = progress_callback or _default_logger

    # ----------- åŸºæœ¬åƒæ•¸ -----------
    kinds = [("hits", "äººæ°£æ¦œ"), ("bestsale", "è¨‚è³¼æ¦œ"), ("pearl", "çç æ¦œ")]
    categories = [("1", "æ„›æƒ…æ–‡è—"), ("2", "è€½ç¾"), ("10", "ç™¾åˆ")]
    periods = [("weekly", "é€±æ¦œ"), ("monthly", "æœˆæ¦œ")]

    dfs: Dict[str, pd.DataFrame] = {}

    for kind, kind_name in kinds:
        for sub, sub_name in categories:
            for period, period_name in periods:
                sheet_name = f"{kind_name}-{sub_name}-{period_name}"
                logger(f"â–¶ é–‹å§‹çˆ¬å–ï¼š{sheet_name}")

                driver = _create_driver()
                try:
                    df = crawl_board(
                        driver, kind, kind_name, sub, sub_name, period, period_name, logger
                    )
                    dfs[sheet_name] = df
                finally:
                    driver.quit()

    # ----------- è¼¸å‡º Excel -----------
    today = datetime.now().strftime("%Y-%m-%d")
    filename = f"popo_æ’è¡Œæ¦œ_{today}.xlsx"
    with pd.ExcelWriter(filename, engine="openpyxl") as writer:
        for sheet, df in dfs.items():
            df.to_excel(writer, sheet_name=sheet[:31], index=False)

    logger(f"ğŸ‰ å·²å®Œæˆä¸¦å„²å­˜ï¼š{filename}")
    return filename

# -----------------------------------------------------------------------------
# CLI åŸ·è¡Œ
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    run_crawler()
