"""
POPO æ’è¡Œæ¦œçˆ¬èŸ²  â€‘ å… webdriverâ€‘manager ç‰ˆ
------------------------------------------------
â€¢ æ”¹ç”¨ **ç³»çµ±å…§å»ºçš„ chromiumâ€‘driver**ï¼Œå®Œå…¨ä¸ä¾è³´ webdriverâ€‘managerï¼Œ
  é¿å… import/ç‰ˆæœ¬ä¸ç›¸å®¹å•é¡Œã€‚
â€¢ ä»ä¿ç•™ `progress_callback` ä¾› Streamlit å³æ™‚å›å ±ã€‚
â€¢ å…¶é¤˜æ ¸å¿ƒé‚è¼¯ï¼ˆåˆ‡æ¦œã€è§£æã€å­˜æª”ï¼‰ç¶­æŒä¸è®Šã€‚
"""

from __future__ import annotations

import time
from datetime import datetime
from pathlib import Path
from typing import Callable, Optional, Dict, List

import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

# Debian 11 (bullseye) å®‰è£ chromium-driver å¾Œçš„é è¨­è·¯å¾‘
CHROMEDRIVER_PATH = "/usr/lib/chromium/chromedriver"
CHROMIUM_BINARY = "/usr/bin/chromium"

# -----------------------------------------------------------------------------
# å…±ç”¨å·¥å…·
# -----------------------------------------------------------------------------

def _default_logger(msg: str) -> None:
    print(msg)


def _create_driver() -> webdriver.Chrome:
    """å»ºç«‹ headless Chromiumï¼Œé›²ç«¯/æœ¬åœ°çš†å¯ç”¨ã€‚"""
    opts = Options()
    opts.binary_location = CHROMIUM_BINARY  # â˜… æŒ‡å®šé›²ç«¯ chromium
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")

    return webdriver.Chrome(
        service=Service(CHROMEDRIVER_PATH),
        options=opts,
    )

# -----------------------------------------------------------------------------
# è§£æå–®æœ¬æ›¸è©³æƒ…
# -----------------------------------------------------------------------------

def get_book_detail(driver: webdriver.Chrome, book_url: str) -> Dict[str, str]:
    driver.get(book_url)
    time.sleep(2)

    # è‡ªå‹•é€šé 18 ç¦é é¢
    if "/limit18" in driver.current_url or "æˆ‘å·²æ»¿18æ­²" in driver.page_source:
        try:
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "a.R-yes"))
            ).click()
            time.sleep(2)
        except Exception as e:
            print(f"[è­¦å‘Š] 18 ç¦é©—è­‰è‡ªå‹•é»æ“Šå¤±æ•—ï¼š{e}")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    tables = soup.find_all("table", class_="book_data")

    detail: Dict[str, str] = {}
    for table in tables:
        for tr in table.find_all("tr"):
            th = tr.find("th")
            td = tr.find("td")
            if th and td:
                detail[th.text.strip()] = td.text.strip()

    return {
        "å…è²»ç« å›": detail.get("å…è²»ç« å›", ""),
        "ä»˜è²»ç« å›": detail.get("ä»˜è²»ç« å›", ""),
        "ç¸½å­—æ•¸": detail.get("ç¸½å­—æ•¸", ""),
        "æ”¶è—æ•¸": detail.get("æ”¶è—æ•¸", ""),
        "è¨‚è³¼æ•¸": detail.get("è¨‚è³¼æ•¸", ""),
    }

# -----------------------------------------------------------------------------
# åˆ‡æ›æ¦œå–® / é€±æœŸè¼”åŠ©
# -----------------------------------------------------------------------------

def switch_board_and_category(driver: webdriver.Chrome, kind: str, sub: str) -> None:
    driver.get("https://www.popo.tw/rank/more")
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, "rank-form1")))
    time.sleep(1)
    js = f"""
    document.getElementById('kind').value ='{kind}';
    document.getElementById('sub').value ='{sub}';
    document.getElementById('rank-form1').submit();
    """
    driver.execute_script(js)
    time.sleep(2)


def switch_rank(driver: webdriver.Chrome, period: str = "weekly") -> None:
    js = f"""
    document.getElementById('type').value='{period}';
    document.getElementById('rank-form1').submit();
    """
    driver.execute_script(js)
    time.sleep(2)

# -----------------------------------------------------------------------------
# çˆ¬å–å–®å¼µæ¦œå–®
# -----------------------------------------------------------------------------

def crawl_board(
    driver: webdriver.Chrome,
    kind: str,
    kind_name: str,
    sub: str,
    sub_name: str,
    period: str,
    period_name: str,
    logger: Callable[[str], None],
) -> pd.DataFrame:
    switch_board_and_category(driver, kind, sub)
    switch_rank(driver, period)

    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "table-rwd"))
    )
    soup = BeautifulSoup(driver.page_source, "html.parser")
    table = soup.find("table", class_="table-rwd")
    rows = table.find("tbody").find_all("tr")

    data: List[Dict[str, str]] = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) == 7:
            book_link = cols[2].find("a", class_="bname")["href"]
            book_url = "https://www.popo.tw" + book_link
            base_info = {
                "æ’è¡Œ": cols[0].text.strip(),
                "é¡åˆ¥": cols[1].text.strip(),
                "æ›¸å": cols[2].text.strip(),
                "æ›¸ç±é€£çµ": book_url,
                "æœ€æ–°ç« å›": cols[3].text.strip(),
                "ä½œè€…": cols[4].text.strip(),
                "å…¬é–‹æ™‚é–“": cols[5].text.strip(),
                "æ›¸ç±ç‹€æ…‹": cols[6].text.strip(),
                "æ¦œå–®": kind_name,
                "åˆ†é¡": sub_name,
                "é€±æœŸ": period_name,
            }
            base_info.update(get_book_detail(driver, book_url))
            data.append(base_info)
            logger(f"âœ” å·²å®Œæˆï¼š{base_info['æ›¸å']} ({kind_name}-{sub_name}-{period_name})")

    return pd.DataFrame(data)

# -----------------------------------------------------------------------------
# å°å¤–ä¸»è¦å…¥å£
# -----------------------------------------------------------------------------

def run_crawler(progress_callback: Optional[Callable[[str], None]] = None) -> str:
    logger = progress_callback or _default_logger

    kinds = [("hits", "äººæ°£æ¦œ"), ("bestsale", "è¨‚è³¼æ¦œ"), ("pearl", "çç æ¦œ")]
    categories = [("1", "æ„›æƒ…æ–‡è—"), ("2", "è€½ç¾"), ("10", "ç™¾åˆ")]
    periods = [("weekly", "é€±æ¦œ"), ("monthly", "æœˆæ¦œ")]

    dfs: Dict[str, pd.DataFrame] = {}

    for kind, kind_name in kinds:
        for sub, sub_name in categories:
            for period, period_name in periods:
                sheet = f"{kind_name}-{sub_name}-{period_name}"
                logger(f"â–¶ é–‹å§‹çˆ¬å–ï¼š{sheet}")

                driver = _create_driver()
                try:
                    df = crawl_board(driver, kind, kind_name, sub, sub_name, period, period_name, logger)
                    dfs[sheet] = df
                finally:
                    driver.quit()

    today = datetime.now().strftime("%Y-%m-%d")
    filename = f"popo_æ’è¡Œæ¦œ_{today}.xlsx"
    with pd.ExcelWriter(filename, engine="openpyxl") as writer:
        for sheet, df in dfs.items():
            df.to_excel(writer, sheet_name=sheet[:31], index=False)

    logger(f"ğŸ‰ å·²å®Œæˆä¸¦å„²å­˜ï¼š{filename}")
    return filename

# -----------------------------------------------------------------------------
# CLI
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    run_crawler()
